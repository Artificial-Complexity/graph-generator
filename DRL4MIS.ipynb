{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "\n",
    "# Check if MPS is available\n",
    "mps_available = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if mps_available else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MISEnv(gym.Env):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(MISEnv, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph = None\n",
    "        self.mis = set()\n",
    "        self.action_space = gym.spaces.Discrete(num_nodes)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(num_nodes * 2,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.graph = nx.gnp_random_graph(self.num_nodes, 0.5)\n",
    "        self.mis = set()\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        node = action\n",
    "        reward = 0\n",
    "        \n",
    "        if node not in self.mis and all(neighbor not in self.mis for neighbor in self.graph.neighbors(node)):\n",
    "            self.mis.add(node)\n",
    "            reward = 1\n",
    "        \n",
    "        done = len(self.mis) + len(set.union(*[set(self.graph.neighbors(n)) for n in self.mis])) == self.num_nodes\n",
    "        \n",
    "        return self._get_observation(), reward, done, False, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = np.zeros(self.num_nodes * 2, dtype=np.float32)\n",
    "        obs[:self.num_nodes] = nx.to_numpy_array(self.graph).sum(axis=1) / (self.num_nodes - 1)  # Normalized degree\n",
    "        obs[self.num_nodes:] = [1 if i in self.mis else 0 for i in range(self.num_nodes)]  # MIS membership\n",
    "        return obs\n",
    "\n",
    "def create_env(num_nodes):\n",
    "    return lambda: MISEnv(num_nodes)\n",
    "\n",
    "def train_agent(env, total_timesteps=100000):\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, \n",
    "                learning_rate=0.0003, \n",
    "                n_steps=2048, \n",
    "                batch_size=64, \n",
    "                n_epochs=10, \n",
    "                gamma=0.99, \n",
    "                gae_lambda=0.95, \n",
    "                clip_range=0.2, \n",
    "                ent_coef=0.01,\n",
    "                device=device)\n",
    "    \n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    return model\n",
    "\n",
    "def evaluate_agent(model, env, num_episodes=100):\n",
    "    mis_sizes = []\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "        mis_sizes.append(len(env.mis))\n",
    "    return np.mean(mis_sizes), np.std(mis_sizes)\n",
    "\n",
    "def visualize_mis(graph, mis):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_color='lightblue', node_size=700)\n",
    "    nx.draw_networkx_nodes(graph, pos, nodelist=list(mis), node_color='red', node_size=800)\n",
    "    plt.title(f\"Graph with MIS (size: {len(mis)})\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent...\n",
      "Using mps device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 258  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008659907 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.39       |\n",
      "|    explained_variance   | -0.0239     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 0.356       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010824912 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.39       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0542     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010279791 |\n",
      "|    clip_fraction        | 0.0905      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.38       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0648     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0806      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009353308 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.37       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0662     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0772      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011220319 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.36       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0569     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.0626      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([create_env(NUM_NODES)])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m train_agent(env, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m mean_mis_size, std_mis_size \u001b[38;5;241m=\u001b[39m evaluate_agent(model, env\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 63\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, total_timesteps)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_agent\u001b[39m(env, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m):\n\u001b[1;32m     52\u001b[0m     model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     53\u001b[0m                 learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0003\u001b[39m, \n\u001b[1;32m     54\u001b[0m                 n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 ent_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     61\u001b[0m                 device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 63\u001b[0m     model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    316\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    317\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    318\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    319\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    320\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    321\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer, n_rollout_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps)\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(obs_tensor)\n\u001b[1;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/policies.py:655\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 655\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    657\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/distributions.py:89\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/distributions.py:298\u001b[0m, in \u001b[0;36mCategoricalDistribution.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/distributions/categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs_2d, sample_shape\u001b[38;5;241m.\u001b[39mnumel(), \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_NODES = 30\n",
    "env = DummyVecEnv([create_env(NUM_NODES)])\n",
    "\n",
    "print(\"Training agent...\")\n",
    "model = train_agent(env, total_timesteps=200000)\n",
    "\n",
    "print(\"Evaluating agent...\")\n",
    "mean_mis_size, std_mis_size = evaluate_agent(model, env.envs[0])\n",
    "print(f\"Average MIS size: {mean_mis_size:.2f} ± {std_mis_size:.2f}\")\n",
    "\n",
    "print(\"Generating example MIS...\")\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "final_graph = env.envs[0].graph\n",
    "final_mis = env.envs[0].mis\n",
    "\n",
    "print(f\"Final MIS size: {len(final_mis)}\")\n",
    "print(\"Visualizing final graph with MIS...\")\n",
    "visualize_mis(final_graph, final_mis)\n",
    "\n",
    "# Compare with NetworkX's implementation\n",
    "nx_mis = nx.maximal_independent_set(final_graph)\n",
    "print(f\"NetworkX MIS size: {len(nx_mis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
